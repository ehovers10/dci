---
title: Dispute in discourse
level: 4
type: chapter
toc: yes
abstract:
  Disputing an utterance can be an important tool in collaborative inquiry. In this paper, I examine a simple dialogue involving dispute and consider two established proposals for analyzing the semantic and pragmatic import of the utterances it contains. I maintain that each is lacking in a well-defined regard and that collaborative update semantics, supplemented with a semantics for bare plurals, offers an alternative that better accounts for the natural interpretation of the dialogue.
---

<!-- Include extras -->
{% include operators.md %}
{% include observations.md %}
{% include examples.md %}
{% include definitions.md %}

# Disputative discourse

Disputes can be frivolous, fractured, and fraudulent. They can derail conversations, and generate so much animosity that perpetuating the dispute itself comes to mean more than the prospect of resolving it.  These disputes may have very little to do with actual conflict over the informational potential of the disputative language used to express them, instead being driven by *ad hominem* and *ad ideologiam* tendencies.

But dispute can also be a fruitful tool for people involved in an honest, cooperative attempt to further mutual knowledge. For one thing, interlocutors sometimes lose track of the cooperative effort, and it is valuable to have a means of correcting them to bring them back on to the task at hand.  But dispute can also play a more central role in the project of inquiry. Corrections, I have claimed, are a crucial tool in properly vetting conjectures in discourse. As such, some element of dispute is central to the enterprise of inquiry. 

I contend that the conjecture/correction discourse pair provides a genuine contribution to inquiry. In the previous chapter, I motivated this status by providing a model of discourse dynamics that carves out a distinctive operation on information that this contribution makes. In this chapter I explore another criterion for genuine a contribution: that it be represented in some conventionalized linguistic tool. I think that this tool is {% include concept.html word="focus," %} and that the virtues of collaborative discourse dynamics come to prominence in explaining focus interpretation when we examine {% include concept.html word="contrastive topic" %} specifically.

The following little discourse, which I label *Bears*, will be the stalking horse for the discussion to follow.

<!-- Grizzly bears -->
{{ grizzly }}

The bracketed material represents {% include concept.html word="focus." base="" %} *Grizzly* in the (b) sentence is marked with a *cf* for {% include concept.html word="contrastive focus." base="contrast" e="" %} We will have more to say about the importance of focus in this dialogue later on. For now, there are a number of interesting pretheoretical observations to be made about the [Bears] dialogue. 

First, the (b) utterance is not merely a dismissal of (a). Instead, it is a natural follow up to, which is to say that (b) {% include concept.html word="coheres" base="coherence" e="" %} with (a). An ability to resolve a coherence relation among a group of utterances in a discourse is a crucial element of interpreting the discourse in context. A particularly salient coherence relation may force an interpretation to light. For instance, consider the following example due to Andrew Kehler {% include ref.html id="kehler2000" o="" p="539" e=":" %}
  
<!-- Presidents -->
{{ presidents }}

The [Presidents](#presidents) dialogue is marked to indicate that it can seem somewhat stilted, resembling two independent sentences rather than a coherent story line. However, if we point out that the subjects in the two sentences can refer to past presidents and interpret the predicates as picking out the subjects' respective publicized traits, we impose a *parallel* relation on the two sentences and uptake goes through much more smoothly.

Kehler suggests that coherence relations imbue a discourse with an additional meaning that goes beyond the sum of its individual parts. He also maintains that linguistic coherence is subject to a Neo-Humean classification based on the basic forms of connection between idea. All connections among ideas, David Hume suggests {% include ref.html id="hume1777" o="" %}, can be reduced to one of three principal such connections: *Resemblance*, *Contiguity* in spacetime, and *Cause/Effect*. Hume's reductive base is perhaps a bit too narrow. If we construre *resemblance* broadly enough, we can bring a wide swath of connections under its umbrella, but it seems to me that there are basic relations among ideas that Hume's reduction leaves out -- in particular, the set of relations corresponding to the *relative generality* of two ideas pertaining to the same domain.[^mereology] The [Bears](#bears) completion is the flip-side of resemblance relations in that it highlights a *distinction* within a class that was previously over-looked. In moving from a coarser to a more granular conception of the sub-space of reality corresponding to beardom, the [Bears](#bears) dialogue exhibits a relation of {% include concept.html word="refinement" base="" e="" %} between the ideas expressed in its utterances.

<!-- Refinement -->
{% include ex.html type="obs" term="refinement" defn=refinement %}

To emphasize the coherent nature of the dialogue, I call the first utterance the {% include concept.html word="initiation" base="" e="" %} and the second utterance the {% include concept.html word="completion." base="" %}

The function of coherence relations is to improve interprebility of the discourse by imposing a restriction on the space of possible conceptions of what the initial speaker is getting at, thereby making the selection procedure for how to respond operate more smoothly. And refinement genuinely does help thin the space of possibilities, as we can see from the following alternative continuation of the [Bears](#bears) initiation:
  
<!-- Druggy bears -->
{{ druggy }}

While perhaps no less true than the original correction, I've marked the completion in [Druggy bears](#druggy-bears) to indicate the increased processing load it carries.[^processingload] This, I suggest, is because the completion expresses a less natural refinement of the discourse domain. As a result, the discourse coherence is reduced.

There is a distinctive, disputative tone in the [Bears](#bears) completion, but it differs from that in, for instance:
  
<!-- Boars -->
{{ boars }}

In [Boars](#boars), the second utterance breaks the flow of inquiry to attempt to fix a perceived error in the preceding discourse. In this example, the disputative *You mean...* is used to signal an attempted {% include concept.html word="repair strategy." base="" %} 

The observation that the [Bears](#bears) dialogue exhibits coherent discourse suggests that the disputative tone of its completion ought not to be treated as signaling a break in the inquiry. Imposing the refinement relation on the discourse eliminates any sense of fault on the part of the initiation in allowing for proper uptake. Instead, we interpret the completion utterance as a {% include concept.html word="correction," base="" e="" %} which is characterized by a dual function of providing: (i) a *denial* of some element of the preceding attempted contribution to discourse and (ii) a *positive contribution* that is related in a salient way to the preceding attempted contribution. 

<!-- Correction -->
{% include ex.html type="obs" term="correction" defn=correction %}

As elements of coherent dialogue involving correction, I refer to sentences such as the initiation in [Bears](#bears) as {% include concept.html word="correctibles." base="correctible" %}

While the completion in the [Bears](#bears) dialogue is a natural one, it certainly isn't the only {% include concept.html word="felicitous" base="felicity" e="" %} response to the initiation. It is open to the responder to challenge the initiation in another way, but interestingly, it is also open to the initiator to retrench in certain ways, as in the following alternate extension of the *Bears* initiation:

<!-- Teddy bears -->  
{{ teddy }}

The retrenchment in (c) strikes my ear as an acceptable response to the challenge in (b). It is similar to the data that Thony Gillies and Kai von Fintel {% include ref.html id="vonfintel2008" o="" %} analyze in regard to retraction of *might* claims. Consider the following, drawn from {% include ref.html id="vonfintel2008" o="n" p="81" e=":" %}

<!-- Keys -->
{{ keys }}

*Might* claims are sometimes retracted when presented with countervailing evidence available in a different context from the original utterance. But as [Keys](#keys) makes clear, *might* claims are ocassionally *resiliant* in the face of such challenges. The felicity of the exchange in [Teddy bears](#teddy-bears) suggests that correctible initiations exhibit similar resilliance.

<!-- Resiliance -->
{% include ex.html type="obs" term="resiliance" defn=resiliance %}

A corrolary to the resiliance of correctible initiations is that completions that involve correction add to the discourse in a specific way. They don't just link back to previous discourse, but change it. That is, corrections are *productive*.

<!-- Productivity -->
{% include ex.html type="obs" term="productivity" defn=productivity %}

The exchange in [Teddy bears](#teddy-bears) reveals an additional interesting fact about disputative discourse. Even if it is appropriate for an initiator to refuse to retract a correctible, the challenge provided in (b) is no less appropriate. The flip-side of the resiliance of correctibles is that they are also highly susceptible to challenge.

<!-- Susceptibility -->
{% include ex.html type="obs" term="susceptibility" sent="Example" sub="Sub-example" defn=susceptibility %}

The dual observations of *resiliance* and *susceptibility* provide an interesting obstacle to an analysis of the semantics and discourse dynamcis of disputative language involving correctibles. *Susceptibility* pushes us toward a more strict representation of the content of the initiation to make sense of the appropriateness of the completion. That is, it seems that utterances (a) and (b) in [Teddy bears](#teddy-bears) represent a disagreement, and the most natural way to formalize disagreement is in terms of presentation of conflicting content. But *resiliance* pushes in the opposite direction. The continuation (c), in so far as it is appropriate, naturally suggests compatibility between the initiation and the completion, which pushes us toward a less strict interpretation of the initiaition content. Navigating between the rock of a correctible's tendency to elicit challenges and the hard place of their ability to withstand those challenges is a particularly interesting puzzle for a linguistic analysis of this type of discourse. But an adequate account should respect all of the observations catalogued above. To make it easier to refer to, I compile the 

<!-- All observations -->
{{ allobs }}

In the remainder of this paper, I examine two extant proposals in the literature and contend that neither captures these observations in their entirety. My preferred analysis is based upon the collaborative update framework, supplementing it with an analysis of {% include concept.html word="bare plurals." base="" e="" %}

# Correction as contrastive topic

A salient feature of the [Bears](#bears) dialogue is the presence of {% include concept.html word="focus" %} on *grizzly* in the completion. Focus is a lingusitic tool whose use serves primarily to *package* information carried by other components. For instance, focus can be be used to divide the informational content of an utterance into that which is assumed to be part of the common ground and that which is newly contributed {% include ref.html id="jackendoff1972" o="n" e="." %} The virtue of information packaging is in aiding uptake for the other participants in the discourse by showing them how the contribution is intended to fit with what has preceeded. 

Well established theories of focus interpretation exist. If one of these could be marshalled to explain the distinctive discourse-dynamical features of [Bears](#bears), we would not have to resort to positing drastic discourse-structural modification. In this section, I explore an alternative semantics account of focus interpretation in terms of its ability to capture the observations noted above.

## Focus and alternatives

One of the most influential accounts of focus interpretation is the *alternative semantics* developed by Mats Rooth {% include ref.html id="rooth1985,rooth1992" o="" e="." %} The idea behind alternative semantics is that focus-sensitive constructions have two semantic values: an *ordinary* semantic value ( {% include sem.html term='&#8729;' index="o" %}) and a *focus* semantic value ( {% include sem.html term='&#8729;' index="f" %}). The ordinary semantic value is whatever linguistic theory minus a theory of focus comes up with. And the focus semantic value for an expression is a set of *alternatives* to the ordinary semantic value. That is, the focus semantic value collects together all elements from the domain of discourse that are type-identical to the focused element of a phrase. More formally:

<!-- Focus semantic values -->
{% include ex.html type="def" term="Focus semantic value" sent="Example" sub="Sub-example" defn=fsem %}

Since focus-related effects can influence interpretation at nearly any level of linguistic analysis, expressions of all types will also be associated with a focus semantic value of the appropriate type. Rooth {% include ref.html id="rooth1985" o="" %} also provides a recursive definition that allows one to derive focus semantic values for expressions containing focused elements as proper parts. As an example, in the case of full sentences, alternative semantics suggests that "the focus semantic value for a phrase of category *S* [sentence] is the set of propositions obtainable from the ordinary semantic value by making a substitution in the position corresponding to the focused phrase'' {% include ref.html id="rooth1992" o="n" p="2" e="." %} 

By adding this additional resource to the semanticist's tool bag, the alternative semantics account of focus promises to provide a unifying explanation of the myriad focus-related interpretive effects. The idea is that the two types of semantic value interact with various semantic and pragmatic rules to give the intended interpretations of focus-sensitive constructions. The principle job of the semanticist is then to discover the requisite rules governing the interaction between the two semantic values. 

## Questions under discussion

A feather in the cap of alternative semantics is the explanation it gives for the phenomenon of question/answer congruence. QA-congruence is exhibited in the following example: 

<!-- Cookie -->
{{ cookie }}

While the (b) answer in [Cookie](#cookie) is a perfectly appropriate response to the question, the (c) answer seems to miss the point of the disucssion proposal made in (a). It would appear that there is a pragmatic rule dictating the form of response that can {% include concept.html word="felicitously" base="felicitous" e="" %} follow a question, and that focus plays a role in the pragmatic process. Roughly, the data suggest that the focused element in a response must correspond to the wh-word of the question to which it is a response.

The alternative semantics for focus provides us with the framework needed to explain the presence of this pragmatic rule. Questions are generally understood to have as their semantic value the set of propositions that constitute answers to the question {% include ref.html id="hamblin1958,groenendijk1984" o="n" e="." %}  So, a (suitably contextually constrained) semantic value for the question in [Cookie](#cookie) might be as in (a) below, which is identical to the focus semantic value of the first response but differs from that of the second (represented in (b) and (c), respectively): 

<!-- Cookie sem -->
{{ cookiesem }}

Thus, we supplement the dual-semantic values framework of alternative semantics with a pragmatic rule to the effect that a felicitous response to a question must have a focus semantic value that matches the ordinary semantic value of the question.[^roothgen] Formally:

<!-- QA-congruence -->
{% include ex.html type="def" term="QA-Congruence" sent="Example" sub="Sub-example" defn=qa %}

Craige Roberts {% include ref.html id="roberts1996" o="" %} develops an integrated theory of pragmatics centered on the question/answer relationship. Her idea is that questions and answers provide the principle organizing structure of discourse. The relevance of a new contribution to the conversation is thus judged in terms of how it fits into that structure. The central order of business in a discourse is to answer the question under discussion (*QUD*). But the path to reaching the answer may not be direct. It can involve providing merely partial anwsers, or it can involve introducing a sub-question, getting the answer to which would aid in getting the answer to the original question.

By whatever path the discourse participants make their way, they are constantly coordinating their contributions around the *QUD*. To be relevant, an assertion must provide at least a partial answer to this question {% include ref.html id="roberts1996" o="n" p="21"%}. Roberts takes questions to have the standard, set of propositions, semantic value, which provides for a natural, set-theoretic account of the partial-answerhood relation. A partial answer to a question *q* is a proposition that entails, for at least one member proposition *q*<sub>i</sub> of the interpretation of the question, either that *q*<sub>i</sub> is true or that it is false. If the dynamical function of asserting is to eliminate certain propositions from the common ground, then the *QUD* framework requires that felicitous assertions eliminate along the contours of the *QUD*. An assertion whose content cross-cuts the partition of the common ground imposed by the QUD fails to provide a relevant contribution to discourse.

The *QUD* model can easily incorporate everything above regarding QA-congruence and focus interpreation. But, not all conversations begin with an explicit question. Many start with an assertion, and seem roll along fruitfully from there. In Roberts' framework, the discourse structure is a theoretical posit --  a set of organizing principles that unify a set of apparently disparate data points drawn from speaker intuitions about semantic content and pragmatic felicity. As such, *QUDs* needen't be drawn from actual utterances in the discourse; it is perfectly possible for them to be implicit in the conversation. While every appropriate assertion must bring us closer to an answer to some question, discourse participants frequently {% include concept.html word="accommodate" base="" e="" %} the question necessary to make sense of an assertion in context.

Accommodation is a powerful tool, but it cannot be comissioned to fix every gap in a theory of pragmatics, and the QUD model seems to have an issue right out of the gate. There are perfectly felicitous responses to questions that do not eliminate cells from the question's partition space.

<!-- Picnic -->
{{ picnic }}

The response in [Picnic](#picnic) involves two focused elements. For both elements, it is easy enough to generate the requisite alternative sets. 

<!-- Picnic sem -->
{{ picnicsem }}

The alternative set generated for the second focused element (marked with a subscripted *f*), is perfectly congruent with the preceding question. But a difficulty arises when we attempt to apply the felicity condition to the element labeled with *cf* for *contrastive focus*. Its alternative set is distinctly not congruent with the preceding question, so we would expect the entire utterance to be marked as infelicitous.

Contrastive focus bears properties that are systematically different from standard focus. In particular, it is realized in English by a distinctive intonation pattern, labeled the B-accent to standard focus' A-accent {% include ref.html id="buering2003" o="n" p="512" e="." %} But the general function of highlighting some element from within a selected set of alternatives seems to be exhibited by focus in all its forms. The key, then, is to locate an antecedent in prior discourse to which the semantic value of the contrastively focus ed element conforms.

Daniel Bu&#x0308;ring {% include ref.html id="buering2003" o="" %} develops a technique to locate the requisite antecedent.  Building on the *QUD* model, Bu&#x0308;ring posits the existence of a *discourse tree*, which generates additional structure in the inquiry. Each question governing the inqury is a node in the tree and is associated with an array of subquestions, complete answers to which constitute partial answers to the parent question.  The idea is that the subquestions provide a _plan of inquiry_ for addressing the parent question. The objective of the inquiry is to answer the top-most question in the tree. The plan for doing so is to answer each of the subquestions in turn.[^stratq] Thus, the implicit structure of the [Picnic](#picnic) can be made explicit as:
  "
<!-- Picnic structure -->
{{ picnicstruc }}

The role of contrastive focus, according to Bu&#x0308;ring, is to mark an utterance as directed at a particular subquestion of the *QUD*. The response in [Picnic](#picnic) is congruent with the (c) subquestion in [Picnic structure](#picnic-structure), and we have the requisite antecedent to explain the felicity of the resposne.

## Advancement or accommodation

Of course, the question of how the antecedent subquestion enters the discourse structure is not a trivial one. It is clearly not something that was there before the response to the original question was made. The scope on felicitous responses involving contrastive topic is too broad for this to be the case:
  
<!-- Chew toys -->
{{ chewtoys }}

That [Chew toys](#chew-toys) can just as easily be read as felicitous suggests that the source of the requisite antecedent is not a subquestion entailed by the speaker's original question. It must, instead, be accommodated by the original speaker in hearing the otherwise relevant response.

The mechanism of accommodation is only loosely understood, but Richmond Thomason's {% include ref.html id="thomason1990" o="" p="343" %} account in terms of obstacle elimination gives us something to grab on to:
  
> Obstacle elimination consists in (1) recognizing the plan of your interlocutor; (2) detecting obstacles to the plan in the form of certain false preconditions of subgoals belonging to the plan; (3) adopting the goal of making these preconditions true; (4) forming a plan to carry this out; and (5) acting on this plan.

A cooperative interlocutor will carry out, to the extent they are able, elimination of obstacles to processing an utterance in the discourse context. With this story in mind, we can see how the original inquirer in [Picnic](#picnic) would reason in their attempt to accommodate their interlocutor's response:
  
> The speaker is attempting to answer my question, and he plans to do so by taking it in parts. But for his answer to be a partial answer to my question, there must be a sub-group of the picinic attendees who are "the children". As such, I will divide the picnic goers along the lines he suggests and impose the resulting structure on my question.

The story will have to be expanded some to bring discourses such as [Bears](#bears), where there isn't even an explicit *QUD* to divide into sub-questions. But it seems clear that an accommodation invoking story could be told for this use of contrastive focus as well.
  
The common knock against appeals to accommodation is that it is too unconstrained. Detractors complain that it evokes the wielding of a magic wand where a more mechanical device ought to be employed. But such a response threatens to enmesh us in a nasty, "one person's trash..." sort of debate. Linguistic communication involves a mixture of domain specific tools and general rationality devices. And we don't have a reliable guide for how to delegate certain interpretive tasks.

The plan-of-inquiry-plus-accommodation story has a distinctive air of plausibility. And letting a general principle of cooperation (obstacle elimination) carry a large interpretive load allows us to maintain a cleaner conversational scoreboard. Yet, I'm swayed by the urge to mechanize the interpretive procedure, and my preferred approach shifts some of the load off of accommodation and back onto the scoreboard. My reason for this is that the accommodation story given above glosses over what I take to be an important function of contrastive focus in the [Bears](#bears). 

As we observed above, the completion utterance in [Bears](#bears) is both [corrective](#correction) and [productive](#productivity). These observations highlight the role of the completion as a driving element in the discourse. But the contrastive-focus-with-accommodation account doubly suppresses the importance of the completion. Focus has a distinctly *backward looking* function; it depends on, and responds to, previous moves in the conversation. So it makes sense that the most common analyses of focus interpretation assign it a *presuppositional* pragmatic function. As such, focus interpretation is reduced to the imposition of a *felicity condition* on the utterance. Pairing the presuppositional account of focus with a principle of accommodation gives the completion a semblance of productive, forward looking power, by having the accommodator introduce the discourse structure needed to interpret the completion. But the particular accommodation story undersells the corrective import of the completion. The responses utterance in [Picnic](#picnic) and [Bears](#bears) function to inform the initiator that their proposed plan of inquiry is too coarse to be effectively carried out. And the corrector offers a substitute, more refined, plan. 

I take this to be an interesting and widespread discourse phenomenon, and one that deserves a more explicit linguistic representation than the accommodation account provides. I'm heartened in my decision to turn away from the lure of accommodation by what I think is a satisfying account of the dynamics of discourses involving contrastive focus, to be developed in the last section. 

# Denial as downdate

Before we get to my preferred account, it is worth exploring another approach to discourses such as [Bears](#bears) that does give pride of linguistic place to the corrective element of the completion utterance.

The completion utterance is distinctively *disputative*. It seems to simultaneously *deny* the original statement and *substitute* an alternative to it. Jennifer Spenader and Emar Maier {% include ref.html id="spenader2009" e="," %} expanding on work of Maier and Bart Geurts {% include ref.html id="geurts1998a,geurtsa" e="," %} attempt to treat corrections as *downdates* on the information state. On this proposal, the function of corrections is to remove from the discourse representation something that was previous added to it. A wrinkle is that corrections do not eliminate information wholesale. Consider, for example:

In this section, I examine the downdate account of denial in terms of its ability to capture the observations about [Bears](#bears) noted earlier.

## Denial and LDRT

In his discussion of negation in its denial making use, Bart Geurts {% include ref.html id="geurts1998a" o="" %} maintains that denial is neither *unitary* nor *swamping*. It is not unitary beacause there are a wide variety of ways in which negation can encode denial, and there is little reason to believe that a single mechanism subsumes tham all. It is not swamping in that negation selectively denies bits of information conveyed by an utterance, granting other bits space in the conversational record.[^facets]

<!-- Movie -->
{{ movie }}

In [Movie](#movie), the initiation carries a packet of information. In the responses, we alternately have denial of an implicature (a), a presupposition (b), and a choice of words (c). In each case, the rest of the packet remains untouched, and may even be available for anaphoric reference later on. 

In standard DRT, extended discourse is represented by a *discourse representation structure* (DRS), which is a pair of sequences of *discourse referents* (dref) and *conditions* (cond). Conditions place constraints on the model against which sentences are evaluated for truth, and drefs serve as the referents of pronouns and other anaphoric expressions invoked in the conditions. Sentences in context contribute conditions and drefs to the DRS by adding them to the appropriate sequence. 

{% capture drsdef %}
{% capture gus %}
{% include ldrt/drs.md %}
{% endcapture %}
{{ gus | markdownify }}
{% endcapture %}
{% include ex.html type="def" term="DRS" sent="Example" sub="Sub-example" defn=drsdef %}

Standard DRT, only defines an *update* function for sentences, but there is nothing preventing us from defining a *downdate*, whereby conditions or drefs are removed from the DRS. But in standard DRT, all information contributed by a sentence is added to the conditions sequence. Thus, removing information removes it wholesale, which is to say that standard DRT makes denial swamping. 

To account for the selective nature of denial, Geurts and Maier {% include ref.html id="geurtsa" o="" %} develop *Layered Discourse Representation Theory* (LDRT). LDRT avoids swamping by indexing all information conveyed by a sentence (both conditions ad drefs) by its kind, which is a matter of the discourse function it performs. The system can recognize such kinds as asserted content (*fr*), presupposed content (*pr*), implicated content (*inf*), contextual information (*k*), and information about syntactic and phonological form (*fm*). The effect is that the information added to a DRS is segmented into different layers. Certain conditions and drefs may get reduplicated in different layers if that information plays multiple roles in the discourse. We will represent the layer to which a bit of information is assigned by a subscript on condition or dref it contributes.

To downdate, then, is to remove information from the DRS, but removing information of the presuppositional variety does nothing to the same information duplicated at the propositional level. The result is non-swamping denial.

There is an additional complication for the denial-as-downdate approach, familiar from the literature on belief revision {% include ref.html id="agm1985,levi1980" o="n" e="." %} An information state is always downdated in response to some impetus. The principal reason for performing a downdate is that the state has just been updated into incoherence, which is to say that an update operation has introduced information that contradicts information already present in the state. The role of the downdate is to remove enough information from the state to restore it to coherence. The problem is that, in general, there are multiple ways to perform this operation.
  
As an example {% include ref.html id="gillies2004" o="n" e="," %} imagine I happen to believe both that at least one of coffeeshop A or B is open (*p*&or;*q*) and that coffeeshop A is open (*p*), which we represent as the deductive closure of the set containing these two beliefs: Cn({ *p*&or;*q*, *p* }). But I subsequently get strong evidence that coffeeshop A is not open, say I see a sign hanging in the door that reads "Closed all day for cleaning". Updating with this new evidence results in the belief state Cn({ *p*&or;*q*, *p*, &not;*p* }), which is just the universal set. I believe too much, and I must revise. But how much do I remove? Minimally, *p* will have to go. But what about *p*&or;*q*? The choice depends on the details of my epistemic situation.
  
Suppose I had seen you walking down the street carrying a cup of coffee. Knowing that there are only two coffee shops in town, I came to believe that at least one of them is open (*p*&or;*q*). Continuing along, I saw lights on in coffeeshop A, which led me to update my belief set with *p*. Only when I got closer did I see the sign. In this case, I have independent reason for my belief that one of the coffeeshops is open, and it seems that *p*&or;*q* deserves to remain. It seems that a *minimal* retraction, excising only the directly contradictory *p*, is the strategy that is called for.

But suppose, instead, that my first bit of evidence is the light coming from coffeeshop A. Since Cn({ *p* }) contains *p*&or;*q*, I similarly have to decide what to do with the disjunction when I read the sign. In this case, the natural response is that it gets swept out along with the removal of *p*. Minimal retraction fails to capture the derivative status of my belief in the disjunction. 

The upshot is that the propositional content of the evidence that initiates the downdate is not sufficient to determine the extent of the downdate; we need, also, a way of representing the source of certain propositions that are candidates for removal. The standard solution is to supplement the information state structure with an *entrenchment relation*, which serves to rank elements of the state in terms of their susceptibility to removal. This puts constraints on how and information state is revised in response to its falling into incoherence: remove the information needed to restore coherence as well as any unentrenched information relevant to the retracted information.[^entrench] Crucially, the entrenchment relation constrains appropriate retraction, but it does not fix a unique revision strategy.

{% capture downdate %}
For DRSs &phi;, &psi;, &chi;,

{% capture subdrslhs %}
&chi; &#x2291; &phi;
{% endcapture %}
{% capture subdrsrhs %}
&exist;m [ ( m &isin; &phi;<sup>d</sup> &and; m &isin; &chi;<sup>d</sup> ) &or; ( m &isin; &phi;<sup>c</sup> &and; m &isin; &chi;<sup>c</sup> ) ]
{% endcapture %}
{% include eqn.html lhs=subdrslhs rhs=subdrsrhs mult=0 conn="iff" %}

{% capture maxlhs %}
&chi; = max(&phi;)
{% endcapture %}
{% capture maxrhs %}
&chi;  &#x2291; &phi; &and; &forall;*c* &isin; &phi;<sup>c</sup> (entrench(*c*) ) &and; &not;&exist;&phi; ( &psi;  &#x2291; &phi; &and; &chi;  &#x2291; &psi; )
{% endcapture %}
{% include eqn.html lhs=maxlhs rhs=maxrhs mult=0 conn="iff" %}

{% capture opluslhs %}
&phi; &#x229E; &psi;
{% endcapture %}
{% capture oplusrhs %}
[ &phi;<sup>d</sup> &oplus; &psi;<sup>d</sup> \| &phi;<sup>c</sup> &oplus; &psi;<sup>c</sup> ]
{% endcapture %}
{% include eqn.html lhs=opluslhs rhs=oplusrhs mult=0 conn="=" %}

{% capture downlhs %}
&phi; <span class="symbol">&darr;</span> &psi;
{% endcapture %}
{% capture downrhs %}
&psi; &oplus; max( &chi; &#x2291; &phi; \| &chi; &#x229E; &phi; &ne; &empty; )
{% endcapture %}
{% include eqn.html lhs=downlhs rhs=downrhs mult=0 conn="=" %}

{% endcapture %}
{% include ex.html type="def" term="Downdate" sent="Example" sub="Sub-example" defn=downdate %}

### Binding retractions

The LDRT approach provides an adequate explanation of certain cases of denial, but it does so by taking incidental advantage of an artefact of the DRT framework. A genuine solution would recognize that the value of DRSs is not that they allow for boundary crossing drefs specifically, but that they manage to interleave bits of information that play different interpretive roles. The dref/condition distinction is one such distinction in roles, but it is not the only one that potentially gives rise to binding-style problems. For example, BAF have shown that interaction between content types is also present for the at-issue/not-at-issue distinction:

The binding problem, originally introduced in a endnote to Karttunen and Peters' *Conventional Implicature* {% include ref.html id="karttunen1979" o="" %}. In the system expounded in that paper, conventional implicature is set apart as a distinct dimension of content from the standard semantic content derived from the meaning of the words in a sentence and the way they are put together. Cleaving implicature from semantic content allows us to explain a number of interesting features about how conventional implicatures work, but it also makes it more difficult to explain the ways in which elements of semantic content can interact with elements of the implicature.

{% include ex.html type="ex" term="Managed" sent="Someone managed to succeed George V on the throne of England." sub="If someone managed to succeed George V on the throne of England, then that country is still a monarchy." defn="Obs/Def" %}

The (a) sentence in [Managed](#managed) seems to implicate that the person who succeeded George V on the throne had difficulty in doing so. But the most natural way of representing the implicature as derived from the utterred sentence is does not capture this.

{% capture manageddef %}

**Semantic content**: &exist;x,y ( george(x) &and; succeeded(y,x) )

**Conventional implicature**: &exist;x,y ( george(x) &and; succeeded(y,x) &and; had-difficulty(x) )

{% include ldrt/managed.html %}
{% endcapture %}
{% include ex.html type="def" term="Managed DRS" sent="Example" sub="Sub-example" defn=manageddef %}

 And it is finding such inroads between severed contents that is termed the binding problem. The separation between levels of content also provides a way around the swamping worry for denial, but because the LDRT approach involves dividing content by way of indexing, we may be concerned that a binding problem will arise here as well. 

But the system has a built in fix that it acquires from the structure of DRSs. In LDRT, while conditions are isolated in their layer, drefs are not. While drefs are introduced in a layer, they transcend that position and are shared by conditions at all layers. Thus, they form a glue that binds layers together just enough to avoid the binding worries.

At least, they avoid binding worries associated with drefs, such as Geurts and Maier's problem with madrigals.

{% include ex.html type="ex" term="Madrigals" sent="Some of the madrigals are nice." sub="" defn="Obs/Def" %}

[Madrigals](#madrigals) carries two implicatures in addition to its assertive content. It implicates first that *not all* of the madrigals are nice and also that the nice madrigals are *merely* nice. The first of these does not give rise to binding concerns, for there is no need to connect the discourse referent intdroduced by the existential quantifier to the variable bound by the implicated not-all quantifier. But the second does; if we represent implicated and asserted content as *fully* separated, then we fail to get capture the constraint that the *merely* nice madrigals are the same as the nice ones.

However, by allowing drefs to transcend layers, we can adequately link the conditions. Here is Geurts and Maier's analysis:[^madrigals]

{% capture madrigalldrtdef %}

{% capture madricaltranslhs %}
Some of the madrigals are nice
{% endcapture %}
{% capture madricaltransrhs %}
[ \| [ x \| madrigal<sub>p</sub> ] &#x27e8;some<sub>a</sub> x &#x27e9; [ \| nice<sub>a</sub>(x) merely-nice<sub>i</sub>(x) ] ]
{% endcapture %}
{% include eqn.html lhs=madricaltranslhs rhs=madricaltransrhs mult=0 conn="&rArr;" %}

{% endcapture %}
{% include ex.html type="def" term="Madrigals LDRS" sent="Example" sub="Sub-example" defn=madrigalldrtdef %}

Jennifer Spenader joins Maier {% include ref.html id="spenader2009" o="" %} in extending this approach to account also for the corrective potential of *contrast*. According to the extension, contrast is a general means of preventing material from entering the common ground. It's presence signifies that the information in its scope is being presented for a downdate operation.

<!-- Spanish -->
{% include ex.html type="ex" term="Spanish" sent="Juan speaks Spanish." sub="Well, he is Argentinian, but he doesn't speak Spanish." defn="Obs/Def" %}

In the completion of [Spanish](#spanish), the choice of connective (*but*) carries contrastive import; the second conjunct, in its scope, is a downdating correction of the initiation sentence. The first conjuct, preceded by the hedge term *well*, provides a concession to the initiation, indicating that the response is intended as a *refinement* rather than a swamping denial.

{% capture spdd %}
{% include ldrt/spanish.html %}
{% endcapture %}
{% include ex.html type="def" term="Spanish LDRT" sent="Example" sub="Sub-example" defn=spdd %}

## Strictness, shifts, and structure

The general collaborative update framework forms the backbone of my preferred analysis of [Bears](#bears) But the specifics of the discourse dynamics are determined by the semantic structure of the particular utterance involved. In this case, the the key elements are the *bare plural* subjects of the utterances. I think these are best Bare plurals are, in this sense, variably strict.

Call a *strict analysis* any translation rules that assign an operator with  universal force to an English sentence. Operators that carry universal force generate true sentences just in case the set determined by the first operand is a subset of the set generated by the second one.

It is natural to offer strict analyses of both *counterfactual conditionals* and sentences with *bare plural* subjects. To say that *Jack would have forgiven him, if Lenny had told the truth* is just to say that all situations in which Lenny tells the truth are also situations in which Jack forgives him. And to say that *Lawyers are liars* is just to say that anything that is a lawyer is also a liar.

<!-- Strict definitions -->
{{ strict }}

But both natural inclinations are held up by sticky situations. Alternative analyses, which rely on the notion of *preferred subclasses* of the set determined by the first operand, are available for both counterfactuals and bare plurals, and it is commonly held that the extraction from the sticky situations requires accepting the alternative analyses. Thony Gillies {% include ref.html id="gillies2007" o="" %} argues that, in the case of counterfactuals, we can have our strictness and extract from the stickiness, too,  by developing an adequate story about the pragmatics of conditionals. He also argues that the extraction is worthwhile. I extend Gillies' ideas to provide what I take to be an equally worthwhile extraction of the strict analysis of bare plurals from their stickiness. My account depends on a pragmatic story told within the framework of collaborative update semantics.

### Counterfactuals

It is natural to offer a strict analysis for counterfectual conditionals. But there is a supposed problem with this analysis. The subset relation is *transitive*, and conjunction is *intersective* which means that strict conditionals verify *thinning*:

<!-- Thinning -->
{{ thinning }}

Unfortunately, [Thinning](#thinning) brings the strict analysis up against a sticky situation. Consider:
  
<!-- Honey -->
{{ honey }}

The antecedent of (b) in [Honey](#honey) is a conjunction involving the antecedent of (a), which means that if (a) is true, the strict analysis predicts (b), whose consequent is the denial of (a)'s, to be false. But the sentences in [Honey](#honey) can be interpreted as a perfectly consistent dialogue. This is a sticky situation. The standard response to examples such as these, developed originally by Robert Stalnaker {% include ref.html id="stalnaker1968" o="" %} and David Lewis {% include ref.html id="lewis1973" o="" e="," %} is to translate counterfactual conditionals with a *variably strict* operator. A variably strict operator works in two stages. First, it appeals to context to *segregate* the set determined by the first operand into preferred and dispreferred classes. It then demands for truth only that the preferred group of elements be a subset of the set determined by the second operand. For counterfactual conditionals, the preferred class is usually glossed as the *closest possible worlds*.

<!-- Variably strict cond -->
{{ variablystrictcond }}

The variably strict analysis avoids the sticky situation because it fails to verify thinning. The closest possible worlds in which Pooh eats less honey need not be a superset of the closest ones in which he both eats less honey and wears his puffy jacket. The ordering relation on the antecedent worlds opens an avenue for the antecedent sets coming apart.

Despite its de-sticking success, Thony Gillies {% include ref.html id="gillies2007" o="" %} has argued that the standard response is not the preferred extraction method. We can preserve the strict analysis of counterfactuals if we allow them to act as monsters {% include ref.html id="kaplan1977" o="n" e="," %} shifting the index of evaluation in the course of interpretation. In particular, if the set determined by the consequent of the conditional is evaluated against a context that is fixed in part by the set determined by the antecedent, then the extraction is achieved.

<!-- Shifty strict conditionals -->
{{ shiftystrictcond }}

The shifty strict analysis captures the same effect as the variably strict analysis. The selection function (&delta;(i)) picks a set of worlds from within the set determined by the antecedent, and the selection is then added to the domain against which the consequent is evaluated. The semantic rule still demands set inclusion between the antecedent worlds and consequent worlds, but the shifted contextual restriction on those sets takes effectively loosens the requirement of strictness. Since there are more worlds included in the set determined by the consequent relative to the domain i<sup>+</sup> than relative to i, the set inclusion demand is more easily met. But since the additionally included worlds are specifically antecedent-relevant, the inclusion demand is not wantonly loose. 

The result is that the evaluation context for the claim that if Pooh had eaten less honey, he would have fit through the hole is different from the evaluation context for the claim that if he had eaten less honey and worn his puffy coat, he would have gotten stuck. And that difference makes a difference. 

The formulation of shifty strict conditionals provided above differs from Gillies' formulation in a couple ways. Primarily, Gillies models evaluation contexts as *hyperdomains* -- nested sets of worlds centered on the evaluation world. The nesting is accomplished by an ordering relation supplied by context. The effect of the context shift enacted by the use of a counterfactual is dictated by this ordering in such a way that the particular set of worlds used to evaluate the counterfactual is the minimal hyperdomain (*closest* set of worlds to the evaluation world) that includes at least some antecedent worlds. Thus, Gillies' framework of choice directly implements the closeness account of counterfactuals.

In contrast, the formulation I have chosen models evaluation domains as information states, and the context shift is enabled by a contextually determined selection function. I chose this formulation in order to highlight the parallel between counterfactual conditionals and my preferred analysis of bare plurals. But there is a lacuna in my formulation. Because there is no ordering on evaluation domains built into the framework, the operation of the selection function is left underspecified. The manner in which worlds are selected from the set of antecedent worlds and added to the evaluation context for the consequent must be fully determined by the context. We thus end up with a messier {% include concept.html word="metasemantics," base="" %} but I think this is preferable. The semantic rules should tell us what is required for truth (or other semantic concepts) but not how to get it. By not building expression-specific formalism (e.g., *hyperdomains*) into the semantic machinery, we keep it more general. Of course, the task of picking a selection function is a large one with which to burden the pragmatic machinery, and I grant that it would be nice to have a more complete story about how this is done. For now, I remain satisfied with specifying constraints on the selection function and leaving its complete definition unspecified.

The other difference between Gillies' and my formulations involves the elements to which the context shift applies. Gillies takes the context shift to be instigated by an *entertainability presupposition* -- that the antecedent be possible relative to the evaluation domain -- carried by the antecedents of conditionals. The presupposition projects up to the whole counterfactual, and it occasionally requires {% include concept.html word="accommodation," base="" %} which is the mechanism by which the shift takes place. So, on Gillies proposal, uttering a counterfactual in context first enacts a context shift, taking us to an evaluation domain that includes the closest antecedent worlds to the evaluation world. Then, the entire counterfactual is evaluated relative to the shifted domain. The formulation I have chosen selectively applies the shifted domain to evaluation of the consequent. I believe that doing so has two benefits. First, it allows for a more integrated application of pragmatic and semantic rules. It needn't be that the semantic rules operate only after the context change rules have been exhaustively applied. The linguistic interpretation system is highly nuanced, and this formulation allows for potential feedback between the modules. Second, a virtue of the collaborative update framework is its ability to represent interlocutors selectively modifying previous contributions to discourse. Thus, the selective application of context shifting fits better with the collaborative framework that will form the basis of our analysis of bare plural sentences.

### Plurals

The natural treatment of sentences involving bare plurals as universally quantified runs into its own sticky situation. Consider:

<!-- Hibernation -->
{{ hibernate }}

Most English speakers take [Hibernation](#hibernation) to be true, even when told that a fair number of bears don't hibernate.[^hibernation] This poses a problem for the strict analysis of bare plurals.

[^hibernation]: [Apparently](https://en.wikipedia.org/wiki/Asian_black_bear#Behaviour), hibernation is driven primarily by climate, and many Asian Black Bears live in warm enough climates to make hibernation unnecessary. Still, nearly all females hibernate when pregnant.

It won't work to loosen the strictness requirement and interpret bare plurals as bound by, say an existential quantifier or one that requires a few witnesses. For that would render true clearly false sentences such as the following:

<!-- Cuddly bears -->
{{ cuddly }}

The existence of Winnie the Pooh and Paddington Bear don't serve to make [Cuddly bears](#cuddly-bears) true. The insight from sentences such as [Hibernation](#hibernation) is not simply that bare plurals carry less than universal force. Instead, the most natural interpretation is that the truth of sentences involving bare plurals is sensitive to the presence of a significant sub-class falling under the plural each member of which verifies the predicate. 

One standard approach to formalizing this idea is to posit a *generic* operator that implements a restricted, universal quantification. As in the variably strict approach to counterfactuals, the *generally universal* account of bare plurals uses context to select a preferred class of individuals within the set determined by the subject and requires for truth that this preferred class be included within the predicate class.[^relevantquant]

[^relevantquant]: This proposal is roughly equivalent to the *Relevant Quantification* approach outlined in {% include ref.html id="pelletierms" o="n" e="." %}

<!-- Generally universal quantifier -->
{{ generics }}

It is perfectly possible for context to restrict membership in the preferred class to hibernating bears, thus explaining the truth of [Hibernation](#hibernate). Of course, it is also possible for context to select only cuddly bears in the case of [Cuddly bears](#cuddly-bears). It is the job of the metasemantics to explain why this selection is far less natural. The generically universal approach thus gets us out of the sticky situation, or at least shifts the extraction duties to pragmatics, which, given the [Resiliance](#resiliance) and [Susceptibility](#susceptibility) of discourse involving bare plurals, seems a not inappropriate delegation of responsibilities.

Despite this de-sticking success, I don't think that the standard response is the correct extraction method. We can preserve the strict analysis of bare plurals [by allowing them to interact with the context in a specific way]. In particular, if the bare plural is evaluated against a context that incorporates the plan of inquiry structure of the discourse, then the extraction can be pulled off.

<!-- Structurally universal quantifier -->
{{ structure }}

The structural analysis is strict in that it demands inclusion of the entire set determined by the bare plural within the properly partitioned predicate set. This is the complete story as far as the semantic component of the interpretive machinery is concerned. The partitioning of the predicate class is the source of the perceived looseness of bare plurals ([Resiliance](#resiliance)), but the mechanism for leveraging that feature belongs to the pragmatic system, to which we now turn. As with the story about shifty strict counterfactuals, the real work comes in getting clear about how the context evolves in response to bare plural utterances. And it is here that the framework of collaborative update semantics earns its keep.

## Pragmatics of plurals and partitions

I assume that plurals carry a *homogeneity presupposition*, characterized as follows:

<!-- Homogeneity -->
{{ homogen }}

That interpretation of plural *definite* noun phrases is subject to a homogeneity constraint is discussed by Sebastian Lo&#x0308;bner, who notes that:

> "The referent of a definite NP cannot be split in case the predicate holds only for some part of it, but not for the whole. Without any differentiating modification of the predication, the alternative is just that of global truth or global falsity. If it is impossible to apply the predicate or its negation globally it fails to yield a truth-value." {% include ref.html id="lobner1987" o="n" p="185" %}
This is why, in reference to a mixed-gender group of young bears engaged in a wrestling match, (a) in the following seems true while (b) seems neither true nor false:
  
<!-- Playing bears -->
{% include ex.html type="ex" term="Bears playing" sent="The cubs are playing." sub="The cubs are boys." defn="Obs/Def" %}

That a homogeneity constraint extends to the case of *bare* plurals, has been suggested by Kai von Fintel (who takes them to be bound by an implicit GEN operator):[^schwarzs]
  
> "A speaker who chooses a sentence involving GEN rather than one of the overt quantifiers signals that it is presupposed that the cases in the domain of quantification are uniform with respect to the property attributed by the scope of the quantifier." {% include ref.html id="vonfintel1997" o="n" p="34" %}

[^schwarzs]: See also Roger Schwarzschild's discussion of distributive contexts for plurals. For Schwarzschild, homogeneity is lexically encoded by the distributivity operator as applied to plurals and falls out of his understanding of plurals in terms of quantification over parts of pluralities combined with a four-valued semantics in which presupposition failure is understood as identity between the positive and negative extensions of a sentence {% include ref.html id="schwarzschild1994" o="" p="222" e="." %}

That the homogeneity constraint should be understood as a *presupposition* tied to plurals is supported by the fact that it is cancelable. 

<!-- Polar bears -->
{{ polar }}

The (b) utterance in [Polar bears](#polar) (adapted from {% include ref.html id="vonfintel1997" o="n" p="34" %}) doesn't presuppose that Polar bears are uniformly white; any such assumption is explicitly eliminated by the nature of the question in (a). And characterization as a presupposition fits the discourse role of homogeneity. Instead of making a claim of individual membership in a class, homogeneity is a property of a class as a whole. Thus, a claim of homogeneity *tests* a set, passing the whole thing through if the test is met and derailing the conversation if it isn't {% include ref.html id="veltman1996" o="n" e="." %} That is exactly what presuppositions do, and in our framework, tests are *structuring updates*.

An additional important feature of [Homogeneity](#homogeneity) is that it does not represent the subject class as uniform *full-stop*. It is only so *with respect to* the predicate class. What this mandates, then, is that the subject class be fully contained within a single cell of a partition defined over the predicate set.

In the framework of collaborative update semantics, the homogeneity presupposition is captured by the presence of a default, trivial partitioning of the common ground, which is itself catalogued by a relation in i<sub>**R**</sub>. If no such partition is present, it is accommodated. The sentence is then evaluated as a universal quantifier evaluated against this partition as outlined in [Structurally universal quantifier](#structurally-universal-quantifier).

But unlike for Gillies' story regarding counterfactuals, which idles if accommodation of the entertainability presupposition fails, the real virtue of the structural universal account of bare plurals shines through when the homogeneity presupposition is challenged, as it is in [Bears](#bears).

### Informal explication

In the framework of collaborative update semantics, the pragmatic process surrounding an utterance of a sentence with a bare plural subject goes, roughly, like this:

The utterance is just like any other in that it initiates a sandbox and presents a proposition for consideration. In this case, the proposition presented is composed of a radical with a default saturation. The plural sentence, as a [correctible](#correction), is essentially incomplete, and must be filled in. The semantics provided above takes bare plural involving sentences to be evaulated against the backdrop of a partition of the domain of individuals. This is what the plural sentence alone lacks, and it is what the homoegeneity presupposition provides. The default saturation is the trivial, homogeneous partition, which divides the domain (relativized to the common ground) into the set of individuals satisfying the plural subject and the set of individuals that fail to. 

If the line of inquiry proposed by the initiation is not accepted, then the conversation derails, and the sandbox is tossed out. But assuming the proposal is picked up by the initiator's interlocutors, there are two avenues for continuing the dialogue:

The presupposition is accommodated, in which case the completion accepts the proposed restriction of the common ground. The worlds that do not represent the set of individuals satisfying the subject as included in the positive partition cell determined by the predicatecloses the sandbox and merges the result into the information state.

The presupposition is rejected (while the line of inquiry is still accepted). At this point, an interlocutor has an opportunity to correct the utterance by offering an alternative saturation of the propositional radical.

The presence of contrastive focus in the completion of [Bears](#bears) serves to indicate the corrective role of the completion utterance. It signifies a denial of the homogeneity presupposition, but because the presupposition is structurally distinct from the propositional radical, this denial does not derail the inquiry. A substitution for homogeneity can be inserted into the surviving propositional radical. [Coherence](#refinement) is retained. 

The substitution is enacted by re-partitioning the domain in a way that carves along the lines of the alternative set determined by focus interpretation. The result is a new presentation of the propositional radical, differently saturated, and the process iterates. 

At the close of collaboration on this issue, the resulting sandbox information state is merged into the primary conversational scoreboard and a new issue can be raised.

### The general formal framework

In collaborative update semantics, the primary update units are *discourse-pairs*, which include (at minimum) an *initiation* and a *completion*. To represent the the integrated functioning of two (or more) utterances issued in serial, we introduce the concept of a *conversational sandbox*. Sandboxes are of the same type as information states, and a sandbox is initiated with much of the current information state copied into it. But the sandbox is isolated from the rest of the information state, so that operations performed within it do not infect the broader state. The effect is that sandboxes can function as a collaborative space, allowing interlocutors to try out various contributions and modify them before committing to their result. When collaboration is complete, the sandbox contents are *merged* into the primary information state, and the update is enacted.

The function of an initiation utterance is two-fold: It *checks out* a new sandbox, and it *commits* a proposition to the inquiry. The checkout opens a new sandbox, copying over (relevant) structural elements (common ground, relation sequence, drefs) from the primary information state. The commit presents the proposition by way of a discourse referent update within the sandbox. Some commits don't lend themselves to collaboration; they elicit either acceptance or denial with no room room for further investigation. But speakers can also commit *correctibles*, which are decomposable into an incomplete, *propositional radical* and a *saturation*. 

Assuming the correctible inspires a corrective effort by a discourse participant, the continuation overrides the initiation commitment, replacing it with a substitution of the original saturation. This process can iterate until all participants are satisfied with the lastest commit. At this point, a completion utterance implements the latest commit, imposing its update effect upon the sandbox information state. Finally, the updated sandbox contents are *merged* into the primary information state.

{% capture gencu %}

**Initiation utterance**
{% capture initlhs %}
U<sub>*init*</sub>
{% endcapture %}
{% capture initrhs %}
[ C<sub>&xi;</sub>(j<sub>&chi;</sub>) &#x23aa; ]<sub>s</sub>
{% endcapture %}
{% include eqn.html lhs=initlhs rhs=initrhs mult=0 conn="&rArr;" %}

**Continuation utterance**
{% capture contlhs %}
U<sub>*cont*</sub>
{% endcapture %}
{% capture contrhs %}
[ &#x27e8; C<sub>&xi;</sub>(k<sub>&chi;</sub>),C<sub>&xi;</sub>(j<sub>&chi;</sub>) &#x27e9; &#x23aa; ]<sub>s</sub>
{% endcapture %}
{% include eqn.html lhs=contlhs rhs=contrhs mult=0 conn="&rArr;" %}

**Completion utterance**
{% capture complhs %}
U<sub>*comp*</sub>
{% endcapture %}
{% capture comprhs %}
[ &#x27e8; C<sub>&xi;</sub>(k<sub>&chi;</sub>),... &#x27e9; &#x23aa; i<sub>s</sub> += \| C<sub>&xi;</sub>(k<sub>&chi;</sub>) \| ; i<sub>**R**</sub> += \| C<sub>&xi;</sub>(k<sub>&chi;</sub>) \| ]<sub>s</sub>
{% endcapture %}
{% include eqn.html lhs=complhs rhs=comprhs mult=0 conn="&rArr;" %}
{% endcapture %}
{% include ex.html type="def" term="General collaborative update" sent="Example" sub="Sub-example" defn=gencu %}


### Plurals specific formal story

All contributions to discourse proceed in the manner of the general formalism outlined above. The specifics of the update procedure depend on the nature of the correctible presented in a sandbox. 

In the case of the [Bears](#bears) dialogue, we treat the evolution of discourse as introducing a series of queries on a *database* and introduce a framework of *relational algebra* to represent the particulars of plural predication.[^maximization]

[^maximization]: Maria Bittner's system of UC<sub>&Omega;</sub>, which builds on the work of van den Berg on plural dynamics, captures many of the same phenomena, though the formulation is couched more centrally in the dynamic semantics tradition. I feel that database theory provides a natural implementation of these ideas. Plus, it strikes me as a potentially fruitful expansion of the update semantics tradition.

#### Basics of relational algebra

A {% include concept.html word="database" base="" e="" %} is a collection of data structured by a series of {% include concept.html word="tables" base="table" e="" %}
that represent interrelations among the points of data. The information contained in a database can be manipulated in a variety of ways principal among which is the {% include concept.html word="query," base="" e="" %} which pulls specific information from one or more tables, packaging it into a {% include concept.html word="view" base="" e="" %}, itself a table in form, but a mere image of the underlying database. Views allow the user to access and make use of data without impacting the underlying structure of the database. 

A {% include concept.html word="relational algebra" base="" e="" %} extends standard set theoretic operations with a set of operations on tables and provides a means of representing manipulations of structured data in a neat way {% include ref.html id="codd1970" o="n" e="." %} For the purposes of representing the discourse evolution of disputative dialogue involving plural predication, we add a subset of the relational algebra framework to collaborative update. Namely, we co-opt operations of: {% include concept.html word="projection" base="" e="," %} {% include concept.html word="outer join" base="" e="," %} {% include concept.html word="grouping" base="group" e="," %} {% include concept.html word="counting" base="count" e="," %} and {% include concept.html word="renaming" base="rename" e="." %}

#### Application to collaborative update
  
The initiation utterance of the [Bears](#bears) dialogue consists of a propositional radical, a plural predication, saturated by a homogeneity presupposition. The utterance serves to open a sandbox, within which the interpretive system treats the predication as a function taking the tables for subject and predicate as input and outputting their *right outer join*. 

The homogeneity presupposition operates on the result of the predication in three stages. First, it implements a structuring update, grouping the table by value of the **sit** attribute. Second, it performs an aggregation and comparisonfunction on the **ent** and **subj** attributes within each group. This consists of counting the number of **ent** of any value, and the number of **subj** of positive value, and comparing them. Groups for for which either the **ent** count equals the **subj** count or the **subj* count is zero pass through; other groups are rejected. The final phase is a restriction, where rejected groups are eliminated from the table.8
{% include bad/index.html rows="7" %}

The use of contrastive focus in the follow up performs three roles. The first two roles pertain to the modification of the subject. First, it introduces the alternative set of the modifier expression. The focus tells us to fill the modifer attribute with values drawn from the alternative set, rather than simple on/off values of an ordinary semantic value. Subject modification is implemented by a function that takes this focus derived modifier table and the unmodified subject table as inputs and outputs their *full outer join*. The full outer join is significant. Since it keeps rows from the modifier table even when there is no shared **ent** in the subject table, as well as vice versa, this is a way for the continuation utterance to genuinely *correct* the initiation. It may add rows to the table for which the original table made no evaluation. 

The second role of contrastive focus is to mark the utterance as a continuation, thus linking it to the previous presentation in the sandbox. We represent this by a *refinement*, in which we **project** just the **sit**, **ent**, and **mod** attributes, renaming the **mod** attribute to **subj**. Thus, the continuation is fully integrated.

{% include contrast/index.html rows="9" %}

Once we have the refined subject in place, we can use it as input to the right outer join implemented by the plural predication of the continuation. The final role of contrastive focus is to overwrite the default homogeneity presupposition, indicating the nature of the structuring update it substitutes -- a partition along the lines of the alternative set of the focused element. This is performed by a grouping operation. In standard plural predication, we only needed to group by the **sit** attribute, but this simplification was made possible by the nature of the values in the **subj** attribute. Since being a bear was taken as an on/off property, and the model determines only its positive extension, there was no need to refine our grouping in order to implement the comparison. The situation is more complex, however, in the case of contrastive plural predication. Here, our presupposition demands that dangerousness be uniform across type of bear. This means that we must additionally group by **subj**
values in order to get the appropriate comparison. We should thus view standard plural predication as a degenerative case of homogeneity presupposition, where there is only one value group within the **subj** attribute.

{% include gbad/index.html rows="7" %}

The final, presupposition-restricted table is stored in the structuring component **R** of the sandbox information state. And it is this relation against which the semantic value of the contribution will be evaluated.

If the corrective continuation is accepted, and no further development is required, the state of the sandbox is merged into the primary information state. Generally, an acceptance will be effected by a nod of agreement or some other *Sounds good!* token. The merge operation involves updating each component of the primary information state with its cognate component from the sandbox state. The primary common ground is intersected with the sandbox common ground, which, recall, was initially seeded with the worlds from the primary common ground. The sandbox table is appended to the primary database. And any drefs introduced in the course of the sandbox procedure are likewise appended to the dref list of the primary information state. Since the propositional commits introduced in the course of the sandbox procedure have at this point either been incorporated into the sandbox table or have been overwritten, it is assumed that these are destroyed with the closing of the sandbox. However, some record of the fact that the sandbox procedure was carried out is valuable. To record this, the merge operation also adds a *version* token to the primary information state, housed in the **XX** component. This serves as a checksum, to allow interlocutors to test discourse integrity over the course of an extended dialogue, and also as a backup marker, which allows interlocutors to restore the conversation at a previous point if things start to go off the rails.

# Outcome and observations obtained

The sticky situation for the universal interpretation of bare plurals is that sentences like [Hibernate](#hibernate) can be true even though there are examples of non-hibernating bears, which is to say the the bare plural sentence can be true even though witnesses to its negation exist in the domain. We are alternately pulled between an inclination to view bare plural quantification as strict and an inclination to loosen it up.

The CU extraction stands behind the strictness inclination. Bare plural sentences involve universal quantification, albeit a partitioned version thereof. The resulting puzzle is to explain (or explain away) the looseness inclination. This ultimately involves giving up on the claims that sentences such as [Hibernate](#hibernate) are true in any rigid sense, but the framework of CU gives us a substitute that makes it easier to let go.

The key observation here is [Susceptibility](#susceptibility). Even if bare plural sentences can be true, their truth seems to be inherently unstable. A shift to a slightly different context 

## References

{% include reflist.html %}

***
{: style="border-bottom:1px solid #aaa;margin:1em;"}

[^mereology]: A philosophical celebrity within the family of relative generality relations is that of *part* to *whole*. That such a relation is a basic logical connection between ideas is emphasized by Goodman and Leonard in their classic explication of the calculus of individuals:
  
    > "The relations of segments of the universe are treated in traditional logistic at two place, first in its theorems concerning the identity and diversity of individuals, and second in its calculus of membership and class-inclusion. But further relations of segments and of classes frequently demand consideration. For example, what is the relation of the class of windows to the class of buildings? No member of either class is a member of the other, nor are any of the segments isolated by the one concept identical with segments isolated by the other. Yet the classes themselves have a very definite relation in that each window is a part of some building. We cannot express this fact in the language of a logistic which lacks a part-whole relation between individuals unless, by making use of some special physical theory, we raise the logical type of each window and each building to the level of a class -- say a class of atoms -- such that any class of atoms that is a window will be included (class-inclusion) in some class that is a building. Such an unforutnate dependence of logical formulation upon the discovery and adoption of a special physical theory, or even upon the presumption that such a suitable theory could in every case be discovered in the course of time, indicates serious deficienceis in the ordinary logistic. Furthermore, a raising of type like that illustrated above is often precluded in a constructional system by other considerations govering the choice of pimitive ideas." {% include ref.html id="goodman1940" o="n" p="45" e="" %}
    
[^processingload]: The claim that the [Druggy bears](#bears) completion does indeed carry a larger processing load is based upon my own interpretation of the discourse.

[^roothgen]: Rooth actually prefers a generalized version of the pragmatic constraint. This allows us to excise any reference to focus semantic values from the semantic theory, isolating it entirely within the pragmatics. For our purposes, the conspicuity of the specific QA-congruence rule is preferable. {% include ref.html id="rooth1992" o="" %}

[^stratq]: How, if at all, does Buring's proposal differ from Roberts' *strategy of inquiry*?

[^facets]: utterances are multi-facted, and corrections tend to mar only a single face.

[^entrench]: We needn't specify the nature of the entrenchment relation here, but we can assume that the introduction of layers gives us a means of further constraining appropriate retraction in the face of incoherence. Certain layers can be assumed to trump others, so that the information on one layer is always more entrenched than information on another. Compare footnote 17 in {% include ref.html id="spenader2009" o="n" %}

[^madrigals]: The representation includes an existence presupposition associated with the existential quantifier. I set aside their representation of the non-universality implicature.
