---
title: Dispute in discourse
level: 4
type: chapter
toc: yes
abstract:
  Disputing an utterance can be an important tool in collaborative inquiry. In this paper, I examine a simple dialogue involving dispute and consider two established proposals for analyzing the semantic and pragmatic import of the utterances it contains. I maintain that each is lacking in a well-defined regard and that collaborative update semantics, supplemented with a semantics for bare plurals, offers an alternative that better accounts for the natural interpretation of the dialogue.
---

<!-- Include extras -->
{% include operators.md %}
{% include observations.md %}
{% include examples.md %}
{% include definitions.md %}

# Disputative discourse

Disputes can be frivolous, fractured, and fraudulent. They can derail conversations, and generate so much animosity that perpetuating the dispute itself comes to mean more than the prospect of resolving it.  These disputes may have very little to do with actual conflict over the informational potential of the disputative language used to express them, instead being driven by *ad hominem* and *ad ideologiam* tendencies.

But dispute can also be a fruitful tool for people involved in an honest, cooperative attempt to further mutual knowledge. For one thing, interlocutors sometimes lose track of the cooperative effort, and it is valuable to have a means of correcting them to bring them back on to the task at hand.  But dispute can also play a more central role in the project of inquiry. Corrections, I have claimed, are a crucial tool in properly vetting conjectures in discourse. As such, some element of dispute is central to the enterprise of inquiry. 

I contend that the conjecture/correction discourse pair provides a genuine contribution to inquiry. In the previous chapter, I motivated this status by providing a model of discourse dynamics that carves out a distinctive operation on information that this contribution makes. In this chapter I explore another criterion for genuine a contribution: that it be represented in some conventionalized linguistic tool. I think that this tool is {% include concept.html word="focus," %} and that the virtues of collaborative discourse dynamics come to prominence in explaining focus interpretation when we examine {% include concept.html word="contrastive topic" %} specifically.

The following little discourse, which I label *Bears*, will be the stalking horse for the discussion to follow.

<!-- Grizzly bears -->
{{ grizzly }}

The bracketed material represents {% include concept.html word="focus." base="" %} *Grizzly* in the (b) sentence is marked with a *cf* for {% include concept.html word="contrastive focus." base="contrast" e="" %} We will have more to say about the importance of focus in this dialogue later on. For now, there are a number of interesting pretheoretical observations to be made about the [Bears] dialogue. 

First, the (b) utterance is not merely a dismissal of (a). Instead, it is a natural follow up to, which is to say that (b) {% include concept.html word="coheres" base="coherence" e="" %} with (a). An ability to resolve a coherence relation among a group of utterances in a discourse is a crucial element of interpreting the discourse in context. A particularly salient coherence relation may force an interpretation to light. For instance, consider the following example due to Andrew Kehler {% include ref.html id="kehler2000" o="" p="539" e=":" %}
  
<!-- Presidents -->
{{ presidents }}

The [Presidents](#presidents) dialogue is marked to indicate that it can seem somewhat stilted, resembling two independent sentences rather than a coherent story line. However, if we point out that the subjects in the two sentences can refer to past presidents and interpret the predicates as picking out the subjects' respective publicized traits, we impose a *parallel* relation on the two sentences and uptake goes through much more smoothly.

Kehler suggests that coherence relations imbue a discourse with an additional meaning that goes beyond the sum of its individual parts. He also maintains that linguistic coherence is subject to a Neo-Humean classification based on the basic forms of connection between idea. All connections among ideas, David Hume suggests {% include ref.html id="hume1777" o="" %}, can be reduced to one of three principal such connections: *Resemblance*, *Contiguity* in spacetime, and *Cause/Effect*. Hume's reductive base is perhaps a bit too narrow. If we construre *resemblance* broadly enough, we can bring a wide swath of connections under its umbrella, but it seems to me that there are basic relations among ideas that Hume's reduction leaves out -- in particular, the set of relations corresponding to the *relative generality* of two ideas pertaining to the same domain.[^mereology] The [Bears](#bears) completion is the flip-side of resemblance relations in that it highlights a *distinction* within a class that was previously over-looked. In moving from a coarser to a more granular conception of the sub-space of reality corresponding to beardom, the [Bears](#bears) dialogue exhibits a relation of {% include concept.html word="refinement" base="" e="" %} between the ideas expressed in its utterances.

[^mereology]: A philosophical celebrity within the family of relative generality relations is that of *part* to *whole*. That such a relation is a basic logical connection between ideas is emphasized by Goodman and Leonard in their classic explication of the calculus of individuals:
  
    > "The relations of segments of the universe are treated in traditional logistic at two place, first in its theorems concerning the identity and diversity of individuals, and second in its calculus of membership and class-inclusion. But further relations of segments and of classes frequently demand consideration. For example, what is the relation of the class of windows to the class of buildings? No member of either class is a member of the other, nor are any of the segments isolated by the one concept identical with segments isolated by the other. Yet the classes themselves have a very definite relation in that each window is a part of some building. We cannot express this fact in the language of a logistic which lacks a part-whole relation between individuals unless, by making use of some special physical theory, we raise the logical type of each window and each building to the level of a class -- say a class of atoms -- such that any class of atoms that is a window will be included (class-inclusion) in some class that is a building. Such an unforutnate dependence of logical formulation upon the discovery and adoption of a special physical theory, or even upon the presumption that such a suitable theory could in every case be discovered in the course of time, indicates serious deficienceis in the ordinary logistic. Furthermore, a raising of type like that illustrated above is often precluded in a constructional system by other considerations govering the choice of pimitive ideas." {% include ref.html id="goodman1940" o="n" p="45" e="" %}

<!-- Refinement -->
{% include ex.html type="obs" term="refinement" defn=refinement %}

To emphasize the coherent nature of the dialogue, I call the first utterance the {% include concept.html word="initiation" base="" e="" %} and the second utterance the {% include concept.html word="completion." base="" %}

The function of coherence relations is to improve interprebility of the discourse by imposing a restriction on the space of possible conceptions of what the initial speaker is getting at, thereby making the selection procedure for how to respond operate more smoothly. And refinement genuinely does help thin the space of possibilities, as we can see from the following alternative continuation of the [Bears](#bears) initiation:
  
<!-- Druggy bears -->
{{ druggy }}

While perhaps no less true than the original correction, I've marked the completion in [Druggy bears](#druggy-bears) to indicate the increased processing load it carries.[^processingload] This, I suggest, is because the completion expresses a less natural refinement of the discourse domain. As a result, the discourse coherence is reduced.

[^processingload]: The claim that the [Druggy bears](#bears) completion does indeed carry a larger processing load is based upon my own interpretation of the discourse.

There is a distinctive, disputative tone in the [Bears](#bears) completion, but it differs from that in, for instance:
  
<!-- Boars -->
{{ boars }}

In [Boars](#boars), the second utterance breaks the flow of inquiry to attempt to fix a perceived error in the preceding discourse. In this example, the disputative *You mean...* is used to signal an attempted {% include concept.html word="repair strategy." base="" %} 

The observation that the [Bears](#bears) dialogue exhibits coherent discourse suggests that the disputative tone of its completion ought not to be treated as signaling a break in the inquiry. Imposing the refinement relation on the discourse eliminates any sense of fault on the part of the initiation in allowing for proper uptake. Instead, we interpret the completion utterance as a {% include concept.html word="correction," base="" e="" %} which is characterized by a dual function of providing: (i) a *denial* of some element of the preceding attempted contribution to discourse and (ii) a *positive contribution* that is related in a salient way to the preceding attempted contribution. 

<!-- Correction -->
{% include ex.html type="obs" term="correction" defn=correction %}

As elements of coherent dialogue involving correction, I refer to sentences such as the initiation in [Bears](#bears) as {% include concept.html word="correctibles." base="correctible" %}

While the completion in the [Bears](#bears) dialogue is a natural one, it certainly isn't the only {% include concept.html word="felicitous" base="felicity" e="" %} response to the initiation. It is open to the responder to challenge the initiation in another way, but interestingly, it is also open to the initiator to retrench in certain ways, as in the following alternate extension of the *Bears* initiation:

<!-- Teddy bears -->  
{{ teddy }}

The retrenchment in (c) strikes my ear as an acceptable response to the challenge in (b). It is similar to the data that Thony Gillies and Kai von Fintel {% include ref.html id="vonfintel2008" o="" %} analyze in regard to retraction of *might* claims. Consider the following, drawn from {% include ref.html id="vonfintel2008" o="n" p="81" e=":" %}

<!-- Keys -->
{{ keys }}

*Might* claims are sometimes retracted when presented with countervailing evidence available in a different context from the original utterance. But as [Keys](#keys) makes clear, *might* claims are ocassionally *resiliant* in the face of such challenges. The felicity of the exchange in [Teddy bears](#teddy-bears) suggests that correctible initiations exhibit similar resilliance.

<!-- Resiliance -->
{% include ex.html type="obs" term="resiliance" defn=resiliance %}

A corrolary to the resiliance of correctible initiations is that completions that involve correction add to the discourse in a specific way. They don't just link back to previous discourse, but change it. That is, corrections are *productive*.

<!-- Productivity -->
{% include ex.html type="obs" term="productivity" defn=productivity %}

The exchange in [Teddy bears](#teddy-bears) reveals an additional interesting fact about disputative discourse. Even if it is appropriate for an initiator to refuse to retract a correctible, the challenge provided in (b) is no less appropriate. The flip-side of the resiliance of correctibles is that they are also highly susceptible to challenge.

<!-- Susceptibility -->
{% include ex.html type="obs" term="susceptibility" sent="Example" sub="Sub-example" defn=susceptibility %}

The dual observations of *resiliance* and *susceptibility* provide an interesting obstacle to an analysis of the semantics and discourse dynamcis of disputative language involving correctibles. *Susceptibility* pushes us toward a more strict representation of the content of the initiation to make sense of the appropriateness of the completion. That is, it seems that utterances (a) and (b) in [Teddy bears](#teddy-bears) represent a disagreement, and the most natural way to formalize disagreement is in terms of presentation of conflicting content. But *resiliance* pushes in the opposite direction. The continuation (c), in so far as it is appropriate, naturally suggests compatibility between the initiation and the completion, which pushes us toward a less strict interpretation of the initiaition content. Navigating between the rock of a correctible's tendency to elicit challenges and the hard place of their ability to withstand those challenges is a particularly interesting puzzle for a linguistic analysis of this type of discourse. But an adequate account should respect all of the observations catalogued above. To make it easier to refer to, I compile the 

<!-- All observations -->
{{ allobs }}

In the remainder of this paper, I examine two extant proposals in the literature and contend that neither captures these observations in their entirety. My preferred analysis is based upon the collaborative update framework, supplementing it with an analysis of {% include concept.html word="bare plurals." base="" e="" %}

# Correction as contrastive topic

A salient feature of the [Bears](#bears) dialogue is the presence of {% include concept.html word="focus" %} on *grizzly* in the completion. Focus is a lingusitic tool whose use serves primarily to *package* information carried by other components as opposed to providing its own contribution. For instance, focus can be be used to divide the informational content of an utterance into that which is assumed to be part of the common ground and that which is newly contributed {% include ref.html id="jackendoff1972" o="n" %}. This presumably aids uptake for the other participants in the discourse by showing them how the contribution is intended to fit with what has preceeded. 

Well established theories of focus interpretation exist. If one of these could be marshalled to explain the distinctive discourse-dynamical features of [Bears](#bears), we would not have to resort to positing drastic discourse-structural modification. In this section, I explore an alternative semantics account of focus interpretation in terms of its ability to capture the observations noted above.

## Focus and alternatives

One of the most influential accounts of focus interpretation is the *alternative semantics* developed by Mats Rooth {% include ref.html id="rooth1985,rooth1992" o="" e="." %} The idea behind alternative semantics is that focus-sensitive constructions have two semantic values, an *ordinary* semantic value ( {% include sem.html term='&#8729;' index="o" %}) and a *focus* semantic value ( {% include sem.html term='&#8729;' index="f" %}). The ordinary semantic value is whatever linguistic theory minus a theory of focus comes up with. And the focus semantic value for an expression is a set of *alternatives* to the ordinary semantic value. That is, the focus semantic value collects together all elements from the domain of discourse that are type-identical to the focused element of a phrase. More formally:

<!-- Focus semantic values -->
{% include ex.html type="def" term="Focus semantic value" sent="Example" sub="Sub-example" defn=fsem %}

Since focus-related effects can influence interpretation at nearly any level of linguistic analysis, expressions of all types will also be associated with a focus semantic value of the appropriate type. Rooth {% include ref.html id="rooth1985" o="" %} also provides a recursive definition that allows one to derive focus semantic values for expressions containing focused elements as proper parts. As an example, in the case of full sentences, alternative semantics suggests that "the focus semantic value for a phrase of category *S* [sentence] is the set of propositions obtainable from the ordinary semantic value by making a substitution in the position corresponding to the focused phrase'' {% include ref.html id="rooth1992" o="n" p="2" e="." %} 

By adding this additional resource to the semanticist's tool bag, the alternative semantics account of focus promises to provide a unifying explanation of the myriad focus-related interpretive effects. The idea is that the two types of semantic value interact with various semantic and pragmatic rules to give the intended interpretations of focus-sensitive constructions. The principle job of the semanticist is then to discover the requisite rules governing the interaction between the two semantic values. 

[^roothgen]: Rooth actually prefers a generalized version of the pragmatic constraint. This allows us to extract any reference to focus semantic values from the semantic theory, isolating it entirely within the pragmatics. For our purposes, the conspicuity of the specific QA-congruence rule is preferable. {% include ref.html id="rooth1992" o="" %}

## Questions under discussion

A principle feather in the cap of alternative semantics is explanation it gives for the phenomenon of question/answer congruence. QA-congruence is exhibited in the following example: 

<!-- Cookie -->
{{ cookie }}


While the (b) answer in [Cookie](#cookie) is a perfectly appropriate response to the question, the (c) answer seems to miss the point of the disucssion proposal made in (a). It would appear that there is a pragmatic rule dictating the form of response that can {% include concept.html word="felicitously" base="felicitous" e="" %} follow a question, and that focus plays a role in the pragmatic process. Roughly, the data suggests that the focused element in a response must correspond to the wh-word of the question to which it is a response.

The alternative semantics for focus provides us with the framework needed to explain the presence of this pragmentic rule. Questions are generally understood to have as their semantic value the set of propositions that constitute answers to the question {% include ref.html id="hamblin1958,groenendijk1984" o="n" e="." %}  So, a (suitably contextually constrained) semantic value for the question in [Cookie](#cookie) might be as in (a) below, which is identical to the focus semantic value of the first response but differs from that of the second (represented in (b) and (c), respectively): 

<!-- Cookie sem -->
{{ cookiesem }}

Thus, we supplement the dual-semantic values framework of alternative semantics with a pragmatic rule to the effect that a felicitous response to a question must have a focus semantic value that matches the ordinary semantic value of the question.[^roothgen] Formally:

<!-- QA-congruence -->
{% include ex.html type="def" term="QA-Congruence" sent="Example" sub="Sub-example" defn=qa %}

Craige Roberts {% include ref.html id="roberts1996" o="" %} develops an integrated theory of pragmatics centered on the question/answer relationship. Her idea is that questions and answers provide the principle organizing structure of discourse. The relevance of a new contribution to the conversation is thus judged in terms of how it fits into that structure. The central order of business in a discourse is to answer the question under discussion (*QUD*). But the path to reaching the answer may not be direct. It can involve providing merely partial anwsers, or it can involve introducing a sub-question, getting the answer to which would aid in getting the answer to the original question.

However, the discourse participants get there, they are constantly coordinating their contributions around the *QUD*. To be relevant, an assertion must provide at least a partial answer to this question {% include ref.html id="roberts1996" o="n" p="21"%}. Roberts takes questions to have the standard, set of propositions, semantic value, which provides for a natural, set-theoretic account of the partial-answerhood relation. A partial answer to a question *q* is a proposition that entails, for at least one member proposition *q*<sub>i</sub> of the interpretation of the question, either that *q*<sub>i</sub> is true or that it is false. If the dynamical function of asserting is to eliminate certain propositions from the common ground, then the *QUD* framework requires that felicitous assertions eliminate along the contours of the *QUD*. An assertion whose content cross-cuts the partition of the common ground imposed by the QUD fails to provide a relevant contribution to discourse.

The *QUD* model can easily incorporate everything above regarding QA-congruence and focus interpreation. But, not all conversations begin with an explicit question. Many start with an assertion, and seem roll along fruitfully from there. In Roberts' framework, the discourse structure is a theoretical posit --  a set of organizing principles that unify a set of apparently disparate data points drawn from speaker intuitions about semantic content and pragmatic felicity. As such, it is perfectly possible for questions to be implicit in the conversation. While every appropriate assertion must bring us closer to an answer to some question, discourse participants frequently {% include concept.html word="accommodate" base="" e="" %} the question necessary to make sense of an assertion in context.

Accommodation is a powerful tool, but it cannot be comissioned to fix every gap in a theory of pragmatics, and the QUD model seems to have an issue right out of the gate. There are perfectly felicitous responses to questions that do not eliminate cells from the question's partition space.

<!-- Picnic -->
{{ picnic }}

The response in [Picnic](#picnic) involves two focused elements. For both elements, it is easy enough to generate the requisite alternative sets. 

<!-- Picnic sem -->
{{ picnicsem }}

The alternative set generated for the second focused element (marked with a subscripted *f*), is perfectly congruent with the preceding question. But a difficulty arises when we attempt to apply the felicity condition to the element labeled with *cf* for *contrastive focus*. Its alternative set is distinctly not congruent with the preceding question, so we would expect the entire utterance to be marked as infelicitous.

**Nature of contrastive focus and difference from normal focus**

Daniel Bu&#x0308;ring {% include ref.html id="buering2003" o="" %} develops a technique to locate the requisite antecedent.  Building on the *QUD* model, Bu&#x0308;ring posits the existence of a *discourse tree*, which generates additional structure in the inquiry. Each question governing the inqury is a node in the tree and is associated with an array of subquestions, complete answers to which constitute partial answers to the parent question.  The idea is that the subquestions provide a _plan of inquiry_ for addressing the parent question. The objective of the inquiry is to answer the top-most question in the tree. The plan for doing so is to answer each of the subquestions in turn.[^stratq]

[^stratq]: How, if at all, does Buring's proposal differ from Roberts' *strategy of inquiry*?

**Represent structure in Picnic example**


## Advancement or accommodation

**Extended accommodation**

This proposal certainly fits with the structure of the question under discussion representation of the conversational scoreboard. But we may be left a little unsatisfied with the details.  First, it seems to be straining the notion of presupposition pretty heavily. Not only is there no direct representation to stand as antecedent to the generated focus semantic value, [AS WAS POINTED OUT EARLIER] contrastive topic is quite resiliant to infellicity.  Many different potential elements could have been fellicitously focused, and it's hard to see how the conversational scoreboard could have all of these options ready to hand. The common response to this sort of worry is to appeal to accommodation. If the scoreboard does not contain the d-tree needed to provide the antecedent for the contrastive focus, then straightaway it comes about. This reliance on accommodation is not without its own set of issues, predominantly stemming from the unconstrained character of accommodation as a theoretical posit.

**Productivity observation**

But what I take to be the central issue with this sort of approach to the problem of contrastive topic in corrections is a conceptual one having to do with the role of corrections in discourse evolution. The question under discussion model of contrastive topic makes the discourse contribution of contrastive topic entirely backward looking. Whatever role the focus plays is just a matter of checking prior discourse for an antecedent to the focus generated alternative set. 

Focus has a distinctly *backward looking* function; it depends on, and responds to, previous moves in the conversation. So it makes sense that the most common analyses of focus interpretation assign it a *presuppositional* pragmatic function. Focus adds a *felicity condition* to the overall import of a sentence; the sentence is interpretable in situ only if material matching the focus semantic value of the sentence can be found in the previously constructed conversation.

Presuppositional analyses are frequently paired with principles of accommodation, which provides them with a semblance of *forward looking* power.

# Denial as downdate

Equally salient in the [Bears](#bears) dialogue is the disputative nature of the response statement. It seems to simultaneously *deny* the original statement and *substitute* an alternative to it. Let's call utterances with this dual function *corrections*. Jennifer Spenader and Emar Maier {% include ref.html id="spenader2009" %}, expanding on work of Maier and Bart Geurts {% include ref.html id="geurts1998a,geurtsa" %}, attempt to treat corrections as *downdating* the information state. On this proposal, the function of corrections is to remove from the discourse representation something that was previous added to it. A wrinkle is that corrections do not eliminate information wholesale. Consider, fo example:

<!-- Movie -->
{{ movie }}

In this section, I examine the downdate account of denial in terms of its ability to capture the observations about [Bears](#bears) noted earlier.

## Layered discourse representation theory



## Information state revision

The issue raised for the correction-as-revision approach is familiar from the literature on belief revision stemming from the work of Gardenfors (in AGM) -- namely, that there are multiple ways to coherently downdate an information state.

The standard solution in belief revision is to supplement the belief state with an *entrenchment relation*, which serves to rank elements of the state (either sentences for the syntactic model or worlds in the semantic model) in terms of their susceptibility to downdate. This puts constraints on how belief state is revised in response to its falling into incoherence. But crucially, the entrenchment relation does not fix a unique revision strategy.

Similarly, the LDRT approach to correction places constraints on downdate by linking drefs via the conditions they bind. But the constraints need not fix a unique downdate strategy. They do so only incidentally in the case of entity type drefs.

It may be suggested that this is a feature rather than a bug in the LDRT proposal, as it allows for a more open-ended and interactive approach to correction. Different conversational participants' scoreboards can diverge based on their different, equally rational, downdate strategies. And this fact helps explain how corrective discourses occasionally degrade into confusion.

I can allow that such a response could be worked out and that it would have certain merits. But it is not an approach I favor, because I am additionally swayed by conceptual motivations for working out a more natural integration of the components of corrective discourse.

I think that the issue reveals the patchy nature of the dref binding fix. Corrective discourse is central enough to inquiry that it calls for an integrated implementation.

## Binding retractions

Revision of an information state takes place when a state degrades into incoherence. The primary source of incoherence is an attempt to update with a proposition whose negation is entailed by the state. Revisions can be analyzed into two components: a downdate, in which the problem proposition as well as a subset of its entailments is removed from the information state; and an update, in which the negation of the proposition is added to the state and the state is appropraitely closed.

**First amendment:** The revision procedure must be amended because corrections do not in general force elimination of utterances whole-sail. Instead, utterances are multi-facted, and corrections tend to mar only a single face, allowing others through unchanged.

To account for this, we add information to our state representations not as information full-stop, but indexed with an information type, where information types indicate the manner in which the information is added to the state. We can think of the information state as being divided into layers based on this indexing. Certain information may be present on multiple layers.

But information state coherence is insensiive to indexing. If a proposition is present on one layer while its negation is present only on a distinct layer, the information state is still incoherent. The layers then provide us with an added means of constraining appropriate revision in the face of incoherence. Certain layers can be assumed to trump others, so that the information on one layer is always more entrenched than information on another.

**Second amendment:** But once information is partitioned in this way, the system is exposed to binding concerns. The binding problem, as it has come to be known, was introduced in a famous footnote to Karttunen and Peters' *Conventional Implicature*. In the system expounded in that paper, conventional implicature is set apart as a distinct dimension of content from the standard semantic content derived from the meaning of the words in a sentence and the way they are put together. Cleaving implicature from semantic content allows us to explain a number of interesting features about how conventional implicatures work, but it also makes it more difficult to explain the ways in which elements of semantic content can interact with elements of the implicature. And it is finding such inroads between severed contents that is termed the binding problem.

Because the amended revision approach involves dividing content by way of indexing, we may be concerned that a binding problem will arise here as well. But the system has a built in fix that it acquires from the structure of DRSs. A discourse representation structure is formally a pair of sets. One set is the universe of the DRS; it contains all the abstract discourse referents (of whatever type) that are introduced in the conversation being represented by the DRS. The other is a set of conditions, which place constraints on elements of the universe in the form of properties and relations holding of and among them. In LDRT, while the conditions are indexed by their layer, the drefs are not. Thus, a single dref can be shared by conditions of different layers. 

# Collaborative update semantics

The general collaborative update framework forms the backbone of the analysis of [Bears](#bears) via the addition of sandboxes and evolving content (modifiable saturation). But the nature of the saturation is determined by the semantic structure of the particular utterance involved.

Bare plurals are treated as generalized quantifiers with universal force subject to variable refinement. Bare plurals are, in this sense, variably strict.

### Strict analyses

Call a *strict analysis* any translation rules that assign an operator with  universal force to an English sentence. Operators that carry universal force generate true sentences just in case the set determined by the first operand is a subset of the set generated by the second one.

It is natural to offer strict analyses of both *counterfactual conditionals* and sentences with *bare plural* subjects. To say that *Jack would have forgiven him, if Lenny had told the truth* is just to say that all situations in which Lenny tells the truth are also situations in which Jack forgives him. And to say that *Lawyers are liars* is just to say that anything that is a lawyer is also a liar.

<!-- Strict definitions -->
{{ strict }}

But both natural inclinations are held up by sticky situations. Alternative analyses, which rely on the notion of *preferred subclasses* of the set determined by the first operand, are available for both counterfactuals and bare plurals, and it is commonly held that the extraction from the sticky situations requires accepting the alternative analyses. Thony Gillies {% include ref.html id="gillies2007" o="" %} argues that, in the case of counterfactuals, we can have our strictness and extract from the stickiness, too,  by developing an adequate story about the pragmatics of conditionals. He also argues that the extraction is worthwhile. I extend Gillies' ideas to provide what I take to be an equally worthwhile extraction of the strict analysis of bare plurals from their stickiness. My account depends on a pragmatic story told within the framework of collaborative update semantics.

### Counterfactuals

It is natural to offer a strict analysis for counterfectual conditionals. But there is a supposed problem with this analysis. The subset relation is *transitive*, and conjunction is *intersective* which means that strict conditionals verify *thinning*:

<!-- Thinning -->
{{ thinning }}

Unfortunately, [Thinning](#thinning) brings the strict analysis up against a sticky situation. Consider:
  
<!-- Honey -->
{{ honey }}

The antecedent of (b) in [Honey](#honey) is a conjunction involving the antecedent of (a), which means that if (a) is true, the strict analysis predicts (b), whose consequent is the denial of (a)'s, to be false. But the sentences in [Honey](#honey) can be interpreted as a perfectly consistent dialogue. This is a sticky situation. The standard response to examples such as these, developed originally by Robert Stalnaker {% include ref.html id="stalnaker1968" o="" %} and David Lewis {% include ref.html id="lewis1973" o="" e="," %} is to translate counterfactual conditionals with a *variably strict* operator. A variably strict operator works in two stages. First, it appeals to context to *segregate* the set determined by the first operand into preferred and dispreferred classes. It then demands for truth only that the preferred group of elements be a subset of the set determined by the second operand. For counterfactual conditionals, the preferred class is usually glossed as the *closest possible worlds*.

<!-- Variably strict cond -->
{{ variablystrictcond }}

The variably strict analysis avoids the sticky situation because it fails to verify thinning. The closest possible worlds in which Pooh eats less honey need not be a superset of the closest ones in which he both eats less honey and wears his puffy jacket. The ordering relation on the antecedent worlds opens an avenue for the antecedent sets coming apart.

Despite its de-sticking success, Thony Gillies {% include ref.html id="gillies2007" o="" %} has argued that the standard response is not the preferred extraction method. We can preserve the strict analysis of counterfactuals if we allow them to act as monsters {% include ref.html id="kaplan1977" o="n" e="," %} shifting the index of evaluation in the course of interpretation. In particular, if the set determined by the consequent of the conditional is evaluated against a context that is fixed in part by the set determined by the antecedent, then the extraction is achieved.

<!-- Shifty strict conditionals -->
{{ shiftystrictcond }}

The shifty strict analysis captures the same effect as the variably strict analysis. The selection function (&delta;(i)) picks a set of worlds from within the set determined by the antecedent, and the selection is then added to the domain against which the consequent is evaluated. The semantic rule still demands set inclusion between the antecedent worlds and consequent worlds, but the shifted contextual restriction on those sets takes effectively loosens the requirement of strictness. Since there are more worlds included in the set determined by the consequent relative to the domain i<sup>+</sup> than relative to i, the set inclusion demand is more easily met. But since the additionally included worlds are specifically antecedent-relevant, the inclusion demand is not wantonly loose. 

The result is that the evaluation context for the claim that if Pooh had eaten less honey, he would have fit through the hole is different from the evaluation context for the claim that if he had eaten less honey and worn his puffy coat, he would have gotten stuck. And that difference makes a difference. 

The formulation of shifty strict conditionals provided above differs from Gillies' formulation in a couple ways. Primarily, Gillies models evaluation contexts as *hyperdomains* -- nested sets of worlds centered on the evaluation world. The nesting is accomplished by an ordering relation supplied by context. The effect of the context shift enacted by the use of a counterfactual is dictated by this ordering in such a way that the particular set of worlds used to evaluate the counterfactual is the minimal hyperdomain (*closest* set of worlds to the evaluation world) that includes at least some antecedent worlds. Thus, Gillies' framework of choice directly implements the closeness account of counterfactuals.

In contrast, the formulation I have chosen models evaluation domains as information states, and the context shift is enabled by a contextually determined selection function. I chose this formulation in order to highlight the parallel between counterfactual conditionals and my preferred analysis of bare plurals. But there is a lacuna in my formulation. Because there is no ordering on evaluation domains built into the framework, the operation of the selection function is left underspecified. The manner in which worlds are selected from the set of antecedent worlds and added to the evaluation context for the consequent must be fully determined by the context. We thus end up with a messier {% include concept.html word="metasemantics," base="" %} but I think this is preferable. The semantic rules should tell us what is required for truth (or other semantic concepts) but not how to get it. By not building expression-specific formalism (e.g., *hyperdomains*) into the semantic machinery, we keep it more general. Of course, the task of picking a selection function is a large one with which to burden the pragmatic machinery, and I grant that it would be nice to have a more complete story about how this is done. For now, I remain satisfied with specifying constraints on the selection function and leaving its complete definition unspecified.

The other difference between Gillies' and my formulations involves the elements to which the context shift applies. Gillies takes the context shift to be instigated by an *entertainability presupposition* -- that the antecedent be possible relative to the evaluation domain -- carried by the antecedents of conditionals. The presupposition projects up to the whole counterfactual, and it occasionally requires {% include concept.html word="accommodation," base="" %} which is the mechanism by which the shift takes place. So, on Gillies proposal, uttering a counterfactual in context first enacts a context shift, taking us to an evaluation domain that includes the closest antecedent worlds to the evaluation world. Then, the entire counterfactual is evaluated relative to the shifted domain. The formulation I have chosen selectively applies the shifted domain to evaluation of the consequent. I believe that doing so has two benefits. First, it allows for a more integrated application of pragmatic and semantic rules. It needn't be that the semantic rules operate only after the context change rules have been exhaustively applied. The linguistic interpretation system is highly nuanced, and this formulation allows for potential feedback between the modules. Second, a virtue of the collaborative update framework is its ability to represent interlocutors selectively modifying previous contributions to discourse. Thus, the selective application of context shifting fits better with the collaborative framework that will form the basis of our analysis of bare plural sentences.

### Plurals

The natural treatment of sentences involving bare plurals as universally quantified runs into its own sticky situation. Consider:

<!-- Hibernation -->
{{ hibernate }}

Most English speakers take [Hibernation](#hibernation) to be true, even when told that a fair number of bears don't hibernate. This poses a problem for the strict analysis of bare plurals.

It won't work to loosen the strictness requirement and interpret bare plurals as bound by, say an existential quantifier or one that requires a few witnesses. For that would render true clearly false sentences such as the following:

<!-- Cuddly bears -->
{{ cuddly }}

The existence of Winnie the Pooh and Paddington Bear don't serve to make [Cuddly bears](#cuddly-bears) true. The insight from sentences such as [Hibernation](#hibernation) is not simply that bare plurals carry less than universal force.

As with counterfactuals, an alternative to the strict analysis exists, and, also as with counterfactuals, the alternative is based upon selection of a preferred class of entities within the set determined by the bare plural.

<!-- Generic quantifier -->
{{ generics }}



## Precision and nuance

This resiliancy is also a defining characteristic of generics:

\ex. \a. Cows give milk.
\b. What about bulls?
\a. I didn't mean $all$ cows!

## References

{% include reflist.html %}

***
{: style="border-bottom:1px solid #aaa;margin:1em;"}
